{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abhijit\n",
      "[nltk_data]     Morye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Abhijit\n",
      "[nltk_data]     Morye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('IMDB Dataset.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub('<[^<]+?>', '', text)\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    # text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['cleaned_review'] = dataset['review'].apply(lambda x: clean_text(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[one, reviewer, mentioned, watching, 1, oz, ep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[wonderful, little, production, filming, techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[basically, there, family, little, boy, jake, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[petter, matteis, love, time, money, visually,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[probably, alltime, favorite, movie, story, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[show, amazing, fresh, innovative, idea, 70, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[encouraged, positive, comment, film, looking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[like, original, gut, wrenching, laughter, lik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "5  Probably my all-time favorite movie, a story o...  positive   \n",
       "6  I sure would like to see a resurrection of a u...  positive   \n",
       "7  This show was an amazing, fresh & innovative i...  negative   \n",
       "8  Encouraged by the positive comments about this...  negative   \n",
       "9  If you like original gut wrenching laughter yo...  positive   \n",
       "\n",
       "                                      cleaned_review  \n",
       "0  [one, reviewer, mentioned, watching, 1, oz, ep...  \n",
       "1  [wonderful, little, production, filming, techn...  \n",
       "2  [thought, wonderful, way, spend, time, hot, su...  \n",
       "3  [basically, there, family, little, boy, jake, ...  \n",
       "4  [petter, matteis, love, time, money, visually,...  \n",
       "5  [probably, alltime, favorite, movie, story, se...  \n",
       "6  [sure, would, like, see, resurrection, dated, ...  \n",
       "7  [show, amazing, fresh, innovative, idea, 70, f...  \n",
       "8  [encouraged, positive, comment, film, looking,...  \n",
       "9  [like, original, gut, wrenching, laughter, lik...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = dataset.loc[:30000, ['review', 'sentiment']]\n",
    "train_features_df = pd.DataFrame(train_features)\n",
    "\n",
    "test_features = dataset.loc[30000:, ['review', 'sentiment']]\n",
    "test_features_df = pd.DataFrame(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_df['sentiment'].replace({'positive': 1, 'negative': -1}, inplace=True)\n",
    "test_features_df['sentiment'].replace({'positive': 1, 'negative': -1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30001, 2), (20000, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_df.shape, test_features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All import \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_features = dataset.loc[:30000, ['review', 'sentiment']]\n",
    "train_features_df = pd.DataFrame(train_features)\n",
    "\n",
    "test_features = dataset.loc[30000:, ['review', 'sentiment']]\n",
    "test_features_df = pd.DataFrame(test_features)\n",
    "\n",
    "train_features_df['sentiment'].replace({'positive': 1, 'negative': -1}, inplace=True)\n",
    "test_features_df['sentiment'].replace({'positive': 1, 'negative': -1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate vectorized form of train_test dataset and vectorized form of whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_vectorizer_matrix(train_features_df, test_features_df):\n",
    "    count_vector = CountVectorizer(analyzer=clean_text)\n",
    "    c_train_features = count_vector.fit_transform(train_features_df['review'])\n",
    "    c_test_features = count_vector.transform(test_features_df['review'])\n",
    "    return c_train_features, c_test_features\n",
    "\n",
    "def tfidf_vectorizer_matrix(train_features_df, test_features_df):\n",
    "    tfidf_vector = TfidfVectorizer(analyzer=clean_text)\n",
    "    tf_train_features = tfidf_vector.fit_transform(train_features_df['review'])\n",
    "    tf_test_features = tfidf_vector.transform(test_features_df['review'])\n",
    "    return tf_train_features, tf_test_features\n",
    "\n",
    "def tfidf_vectorizer_whole_dataset(datatset):\n",
    "    tf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "    tf_X_featues = tf_vect.fit_transform(dataset['review'])\n",
    "    return tf_X_featues\n",
    "    \n",
    "def count_vectorizer_whole_dataset(dataset):\n",
    "    count_vect = CountVectorizer(analyzer=clean_text)\n",
    "    count_X_featues = count_vect.fit_transform(dataset['review'])\n",
    "    return count_X_featues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_preciosn_recall_fscore_accuracy(true_value, predicted_value):\n",
    "    precison, recall, fscore, support = precision_recall_fscore_support([row for row in true_value['sentiment'].values.tolist()], predicted_value, average='binary')\n",
    "    print(f'Precison - {np.round((precison * 100), 2)}')\n",
    "    print(f'Recall - {np.round((recall * 100), 2)}')\n",
    "    accuracy = ((true_value['sentiment'].values == predicted_value).sum()/len(predicted_value))*100\n",
    "    print(f\"Accuracy - {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes using Hold out dataset (Train-Test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(train_features, train_dataframe, test_features):    \n",
    "    model = MultinomialNB()\n",
    "    model.fit(train_features, [row for row in train_dataframe['sentiment']])\n",
    "    predicted_values = model.predict(test_features)\n",
    "    return predicted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison - 86.74\n",
      "Recall - 83.7\n",
      "Accuracy - 85.475\n"
     ]
    }
   ],
   "source": [
    "c_train_features, c_test_features = count_vectorizer_matrix(train_features_df, test_features_df)\n",
    "nb_predicted_values = naive_bayes(c_train_features, train_features_df, c_test_features)\n",
    "calculate_preciosn_recall_fscore_accuracy(test_features_df, nb_predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison - 87.0\n",
      "Recall - 84.71\n",
      "Accuracy - 86.045\n"
     ]
    }
   ],
   "source": [
    "tf_train_features, tf_test_features = tfidf_vectorizer_matrix(train_features_df, test_features_df)\n",
    "nb_tf_predicted_values = naive_bayes(tf_train_features, train_features_df, tf_test_features)\n",
    "calculate_preciosn_recall_fscore_accuracy(test_features_df, nb_tf_predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Using K_fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_k_fold(X_features, dataset):\n",
    "    kfold_nb = MultinomialNB()\n",
    "    kfold = KFold(n_splits=5)\n",
    "    result = cross_val_score(kfold_nb, X_features, dataset['sentiment'], cv=kfold, n_jobs=-1, scoring='accuracy')\n",
    "    print(f'Average Accuracy - {(result.sum()/len(result))*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy - 85.86600000000001\n"
     ]
    }
   ],
   "source": [
    "count_X_features = count_vectorizer_whole_dataset(dataset)\n",
    "naive_bayes_k_fold(count_X_features, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy - 86.53800000000001\n"
     ]
    }
   ],
   "source": [
    "tfidf_X_features = tfidf_vectorizer_whole_dataset(dataset)\n",
    "naive_bayes_k_fold(tfidf_X_features, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Paramter Tuning for Naive Bayes model using alpha as hyper parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    alpha : float, default=1.0\n",
    "    Additive (Laplace/Lidstone) smoothing parameter\n",
    "    (0 for no smoothing).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(alpha, train_features, test_features, train_features_df, test_features_df):\n",
    "    grid_nb = MultinomialNB(alpha=alpha)\n",
    "    grid_nb_model = grid_nb.fit(train_features, [row for row in train_features_df['sentiment']])\n",
    "    predicted_y = grid_nb_model.predict(test_features)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support([row for row in test_features_df['sentiment'].values.tolist()], predicted_y, average='binary')\n",
    "    print(f\"For alpha - {alpha}\")\n",
    "    print(f'Precison - {np.round((precision * 100), 2)}')\n",
    "    print(f'Recall - {np.round((recall * 100), 2)}')\n",
    "    accuracy = ((test_features_df['sentiment'].values == predicted_y).sum()/len(predicted_y))*100\n",
    "    print(f\"Accuracy - {accuracy}\")\n",
    "    print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using cout vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha - 1\n",
      "Precison - 86.74\n",
      "Recall - 83.7\n",
      "Accuracy - 85.475\n",
      "****************************************************************************************************\n",
      "For alpha - 2\n",
      "Precison - 86.57\n",
      "Recall - 83.6\n",
      "Accuracy - 85.33500000000001\n",
      "****************************************************************************************************\n",
      "For alpha - 3\n",
      "Precison - 86.45\n",
      "Recall - 83.55\n",
      "Accuracy - 85.245\n",
      "****************************************************************************************************\n",
      "For alpha - 4\n",
      "Precison - 86.36\n",
      "Recall - 83.55\n",
      "Accuracy - 85.195\n",
      "****************************************************************************************************\n",
      "For alpha - 5\n",
      "Precison - 86.17\n",
      "Recall - 83.57\n",
      "Accuracy - 85.1\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for alpha in [1, 2, 3, 4, 5]:\n",
    "    grid_search(alpha, c_train_features, c_test_features, train_features_df, test_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha - 1\n",
      "Precison - 87.0\n",
      "Recall - 84.71\n",
      "Accuracy - 86.045\n",
      "****************************************************************************************************\n",
      "For alpha - 2\n",
      "Precison - 87.47\n",
      "Recall - 84.11\n",
      "Accuracy - 86.05000000000001\n",
      "****************************************************************************************************\n",
      "For alpha - 3\n",
      "Precison - 87.9\n",
      "Recall - 83.47\n",
      "Accuracy - 86.00999999999999\n",
      "****************************************************************************************************\n",
      "For alpha - 4\n",
      "Precison - 88.02\n",
      "Recall - 82.91\n",
      "Accuracy - 85.835\n",
      "****************************************************************************************************\n",
      "For alpha - 5\n",
      "Precison - 88.22\n",
      "Recall - 82.54\n",
      "Accuracy - 85.78\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for alpha in [1, 2, 3, 4, 5]:\n",
    "    grid_search(alpha, tf_train_features, tf_test_features, train_features_df, test_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest using Hold Out(Train/Test dataset) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(train_features, train_dataframe, test_features):    \n",
    "    rf_model = RandomForestClassifier(n_jobs=-1)\n",
    "    rf_model.fit(train_features, [row for row in train_dataframe['sentiment']])\n",
    "    predicted_values = rf_model.predict(test_features)\n",
    "    return predicted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using count vecorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison - 85.29\n",
      "Recall - 85.22\n",
      "Accuracy - 85.285\n"
     ]
    }
   ],
   "source": [
    "count_rf_predicted_values = random_forest(c_train_features, train_features_df, c_test_features)\n",
    "calculate_preciosn_recall_fscore_accuracy(test_features_df, count_rf_predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison - 85.39\n",
      "Recall - 84.7\n",
      "Accuracy - 85.125\n"
     ]
    }
   ],
   "source": [
    "tf_rf_predicted_values = random_forest(tf_train_features, train_features_df, tf_test_features)\n",
    "calculate_preciosn_recall_fscore_accuracy(test_features_df, tf_rf_predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest using K_Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_k_fold(X_features, dataset):\n",
    "    rf_kfold = RandomForestClassifier(n_jobs=-1)\n",
    "    kfold = KFold(n_splits=5)\n",
    "    result = cross_val_score(rf_kfold, X_features, dataset['sentiment'], cv=kfold, n_jobs=-1, scoring='accuracy')\n",
    "    print(f'Average Accuracy - {(result.sum()/len(result))*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy - 85.54599999999999\n"
     ]
    }
   ],
   "source": [
    "random_forest_k_fold(count_X_features, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy - 85.13\n"
     ]
    }
   ],
   "source": [
    "random_forest_k_fold(tfidf_X_features, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_random_forest(estimator, depth, train_features, test_features, train_features_df, test_features_df):\n",
    "    grid_rf = RandomForestClassifier(n_estimators=estimator, max_depth=depth, n_jobs=-1)\n",
    "    grid_rf_model = grid_rf.fit(train_features, [row for row in train_features_df['sentiment']])\n",
    "    predicted_y = grid_rf_model.predict(test_features)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support([row for row in test_features_df['sentiment'].values.tolist()], predicted_y, average='binary')\n",
    "    print(f\"For estimator - {estimator} and depth - {depth}\")\n",
    "    print(f'Precison - {np.round((precision * 100), 2)}')\n",
    "    print(f'Recall - {np.round((recall * 100), 2)}')\n",
    "    accuracy = ((test_features_df['sentiment'].values == predicted_y).sum()/len(predicted_y))*100\n",
    "    print(f\"Accuracy - {accuracy}\")\n",
    "    print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For estimator - 100 and depth - 10\n",
      "Precison - 80.66\n",
      "Recall - 87.41\n",
      "Accuracy - 83.25\n",
      "****************************************************************************************************\n",
      "For estimator - 100 and depth - 20\n",
      "Precison - 81.4\n",
      "Recall - 87.06\n",
      "Accuracy - 83.60499999999999\n",
      "****************************************************************************************************\n",
      "For estimator - 100 and depth - 30\n",
      "Precison - 82.93\n",
      "Recall - 87.67\n",
      "Accuracy - 84.83500000000001\n",
      "****************************************************************************************************\n",
      "For estimator - 100 and depth - 50\n",
      "Precison - 83.82\n",
      "Recall - 86.57\n",
      "Accuracy - 84.95\n",
      "****************************************************************************************************\n",
      "For estimator - 100 and depth - None\n",
      "Precison - 85.21\n",
      "Recall - 84.8\n",
      "Accuracy - 85.06\n",
      "****************************************************************************************************\n",
      "For estimator - 200 and depth - 10\n",
      "Precison - 80.2\n",
      "Recall - 89.13\n",
      "Accuracy - 83.59\n",
      "****************************************************************************************************\n",
      "For estimator - 200 and depth - 20\n",
      "Precison - 82.5\n",
      "Recall - 87.9\n",
      "Accuracy - 84.65\n",
      "****************************************************************************************************\n",
      "For estimator - 200 and depth - 30\n",
      "Precison - 83.53\n",
      "Recall - 88.08\n",
      "Accuracy - 85.38\n",
      "****************************************************************************************************\n",
      "For estimator - 200 and depth - 50\n",
      "Precison - 84.3\n",
      "Recall - 88.04\n",
      "Accuracy - 85.845\n",
      "****************************************************************************************************\n",
      "For estimator - 200 and depth - None\n",
      "Precison - 86.07\n",
      "Recall - 86.62\n",
      "Accuracy - 86.32\n",
      "****************************************************************************************************\n",
      "For estimator - 300 and depth - 10\n",
      "Precison - 81.34\n",
      "Recall - 88.18\n",
      "Accuracy - 84.0\n",
      "****************************************************************************************************\n",
      "For estimator - 300 and depth - 20\n",
      "Precison - 82.5\n",
      "Recall - 89.05\n",
      "Accuracy - 85.105\n",
      "****************************************************************************************************\n",
      "For estimator - 300 and depth - 30\n",
      "Precison - 83.74\n",
      "Recall - 88.63\n",
      "Accuracy - 85.735\n",
      "****************************************************************************************************\n",
      "For estimator - 300 and depth - 50\n",
      "Precison - 84.96\n",
      "Recall - 87.77\n",
      "Accuracy - 86.13499999999999\n",
      "****************************************************************************************************\n",
      "For estimator - 300 and depth - None\n",
      "Precison - 85.82\n",
      "Recall - 87.34\n",
      "Accuracy - 86.47500000000001\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for est in [100, 200, 300]:\n",
    "    for dep in [10, 20, 30, 50, None]:\n",
    "        grid_search_random_forest(est, dep, c_train_features, c_test_features, train_features_df, test_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For estimator - 100 and depth - 10\n",
      "Precison - 79.55\n",
      "Recall - 87.38\n",
      "Accuracy - 82.485\n",
      "****************************************************************************************************\n",
      "For estimator - 100 and depth - 20\n",
      "Precison - 81.94\n",
      "Recall - 86.88\n",
      "Accuracy - 83.89\n",
      "****************************************************************************************************\n",
      "For estimator - 100 and depth - 30\n",
      "Precison - 82.63\n",
      "Recall - 86.3\n",
      "Accuracy - 84.10499999999999\n",
      "****************************************************************************************************\n",
      "For estimator - 100 and depth - 50\n",
      "Precison - 83.79\n",
      "Recall - 86.07\n",
      "Accuracy - 84.735\n",
      "****************************************************************************************************\n",
      "For estimator - 100 and depth - None\n",
      "Precison - 85.45\n",
      "Recall - 84.68\n",
      "Accuracy - 85.15\n",
      "****************************************************************************************************\n",
      "For estimator - 200 and depth - 10\n",
      "Precison - 80.17\n",
      "Recall - 87.51\n",
      "Accuracy - 82.955\n",
      "****************************************************************************************************\n",
      "For estimator - 200 and depth - 20\n",
      "Precison - 82.46\n",
      "Recall - 87.89\n",
      "Accuracy - 84.61999999999999\n",
      "****************************************************************************************************\n",
      "For estimator - 200 and depth - 30\n",
      "Precison - 83.23\n",
      "Recall - 87.09\n",
      "Accuracy - 84.795\n",
      "****************************************************************************************************\n",
      "For estimator - 200 and depth - 50\n",
      "Precison - 84.33\n",
      "Recall - 87.03\n",
      "Accuracy - 85.45\n",
      "****************************************************************************************************\n",
      "For estimator - 200 and depth - None\n",
      "Precison - 85.49\n",
      "Recall - 85.58\n",
      "Accuracy - 85.55\n",
      "****************************************************************************************************\n",
      "For estimator - 300 and depth - 10\n",
      "Precison - 81.14\n",
      "Recall - 88.47\n",
      "Accuracy - 83.975\n",
      "****************************************************************************************************\n",
      "For estimator - 300 and depth - 20\n",
      "Precison - 82.81\n",
      "Recall - 87.32\n",
      "Accuracy - 84.61999999999999\n",
      "****************************************************************************************************\n",
      "For estimator - 300 and depth - 30\n",
      "Precison - 83.48\n",
      "Recall - 87.27\n",
      "Accuracy - 85.02499999999999\n",
      "****************************************************************************************************\n",
      "For estimator - 300 and depth - 50\n",
      "Precison - 84.51\n",
      "Recall - 87.01\n",
      "Accuracy - 85.55\n",
      "****************************************************************************************************\n",
      "For estimator - 300 and depth - None\n",
      "Precison - 86.12\n",
      "Recall - 86.06\n",
      "Accuracy - 86.115\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for est in [100, 200, 300]:\n",
    "    for dep in [10, 20, 30, 50, None]:\n",
    "        grid_search_random_forest(est, dep, tf_train_features, tf_test_features, train_features_df, test_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning using grid search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest_Kfold_cv(estimator, depth, X_features, dataset):\n",
    "    kf_rf = RandomForestClassifier(n_estimators=estimator, max_depth=depth, n_jobs=-1)\n",
    "    kf = KFold(n_splits=5)\n",
    "    cv_results = cross_val_score(kf_rf, X_features, dataset['sentiment'], n_jobs=-1, cv=kf)\n",
    "    print(f'For estimator {estimator}  and depth {depth}')\n",
    "    print(f'Average Accuracy - {(cv_results.sum()/len(cv_results))*100}')\n",
    "    print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For estimator 100  and depth 10\n",
      "Average Accuracy - 82.38999999999999\n",
      "****************************************************************************************************\n",
      "For estimator 100  and depth 20\n",
      "Average Accuracy - 84.27600000000001\n",
      "****************************************************************************************************\n",
      "For estimator 100  and depth 30\n",
      "Average Accuracy - 84.80400000000002\n",
      "****************************************************************************************************\n",
      "For estimator 100  and depth 50\n",
      "Average Accuracy - 85.23399999999998\n",
      "****************************************************************************************************\n",
      "For estimator 100  and depth None\n",
      "Average Accuracy - 85.68599999999999\n",
      "****************************************************************************************************\n",
      "For estimator 200  and depth 10\n",
      "Average Accuracy - 83.75399999999999\n",
      "****************************************************************************************************\n",
      "For estimator 200  and depth 20\n",
      "Average Accuracy - 84.98800000000001\n",
      "****************************************************************************************************\n",
      "For estimator 200  and depth 30\n",
      "Average Accuracy - 85.50800000000001\n",
      "****************************************************************************************************\n",
      "For estimator 200  and depth 50\n",
      "Average Accuracy - 86.05199999999999\n",
      "****************************************************************************************************\n",
      "For estimator 200  and depth None\n",
      "Average Accuracy - 86.48\n",
      "****************************************************************************************************\n",
      "For estimator 300  and depth 10\n",
      "Average Accuracy - 84.15000000000002\n",
      "****************************************************************************************************\n",
      "For estimator 300  and depth 20\n",
      "Average Accuracy - 85.28400000000002\n",
      "****************************************************************************************************\n",
      "For estimator 300  and depth 30\n",
      "Average Accuracy - 85.742\n",
      "****************************************************************************************************\n",
      "For estimator 300  and depth 50\n",
      "Average Accuracy - 86.272\n",
      "****************************************************************************************************\n",
      "For estimator 300  and depth None\n",
      "Average Accuracy - 86.56599999999999\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for est in [100, 200, 300]:\n",
    "    for dep in [10, 20, 30, 50, None]:\n",
    "        randomForest_Kfold_cv(est, dep, count_X_features, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For estimator 100  and depth 10\n",
      "Average Accuracy - 82.10000000000001\n",
      "****************************************************************************************************\n",
      "For estimator 100  and depth 20\n",
      "Average Accuracy - 83.34400000000001\n",
      "****************************************************************************************************\n",
      "For estimator 100  and depth 30\n",
      "Average Accuracy - 84.19000000000001\n",
      "****************************************************************************************************\n",
      "For estimator 100  and depth 50\n",
      "Average Accuracy - 84.78000000000002\n",
      "****************************************************************************************************\n",
      "For estimator 100  and depth None\n",
      "Average Accuracy - 85.14600000000002\n",
      "****************************************************************************************************\n",
      "For estimator 200  and depth 10\n",
      "Average Accuracy - 82.97799999999998\n",
      "****************************************************************************************************\n",
      "For estimator 200  and depth 20\n",
      "Average Accuracy - 84.386\n",
      "****************************************************************************************************\n",
      "For estimator 200  and depth 30\n",
      "Average Accuracy - 85.07400000000001\n",
      "****************************************************************************************************\n",
      "For estimator 200  and depth 50\n",
      "Average Accuracy - 85.612\n",
      "****************************************************************************************************\n",
      "For estimator 200  and depth None\n",
      "Average Accuracy - 86.06\n",
      "****************************************************************************************************\n",
      "For estimator 300  and depth 10\n",
      "Average Accuracy - 83.60199999999999\n",
      "****************************************************************************************************\n",
      "For estimator 300  and depth 20\n",
      "Average Accuracy - 84.726\n",
      "****************************************************************************************************\n",
      "For estimator 300  and depth 30\n",
      "Average Accuracy - 85.338\n",
      "****************************************************************************************************\n",
      "For estimator 300  and depth 50\n",
      "Average Accuracy - 85.82000000000001\n",
      "****************************************************************************************************\n",
      "For estimator 300  and depth None\n",
      "Average Accuracy - 86.298\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for est in [100, 200, 300]:\n",
    "    for dep in [10, 20, 30, 50, None]:\n",
    "        randomForest_Kfold_cv(est, dep, tfidf_X_features, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For final modeling, we will use Count vectorizer with Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_features_df):\n",
    "    count_vector = CountVectorizer(analyzer=clean_text)\n",
    "    c_train_features = count_vector.fit_transform(train_features_df['review'])\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_fit = nb_model.fit(c_train_features, [row for row in train_features_df['sentiment']])\n",
    "    return count_vector, nb_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_classifier(review, vector, model_obj):\n",
    "    review_df  = pd.DataFrame([review], columns=['review'])\n",
    "    test_features = vector.transform(review_df['review'])\n",
    "    result = model_obj.predict(test_features)\n",
    "    if result == 1:\n",
    "        print(\"Given review shows positive sentiment\")\n",
    "    elif result == -1:\n",
    "        print('Given review shows negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vector, nb_model = model(train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This film has everything you want in a team up super hero film from awesome action sequences to great characters but still has some noticeable flaws.\\n\\nThe fight choreography was great not because of john wick level action sequences but just really fun and enjoyable action scemes that really embrace comic book fun.\\nThe villain loki is perfect as although he may have a generic motavation, the way clashes with the heroes ideologies and has huge amounts of personality made him unforgettable. \\n\\nOne of my favourite things about this movie is the sense of urgency and dread, it really does feel like anything could happen and some thing could go wrong and this is due to some great writing and acting. another thing i love is that the characters work so well with each oher and are handled so well, the best avengers (not in power but in character complexity) are all focused on more than the more boring characters which is awesome as it feels no potential was lost in the characters and it shows they actually put thought into there characters.\\n\\nThis movie isnt perfect though as the cinematography which was dissapointing and felt really bland and ugly to look at as it felt like they dont use there unique set design.\\n\\nOverall this movie still holds up as one of the best and most unforgettable mcu experiences but was still dissapionting in some aspects.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_review = '''This film has everything you want in a team up super hero film from awesome action sequences to great characters but still has some noticeable flaws.\n",
    "\n",
    "The fight choreography was great not because of john wick level action sequences but just really fun and enjoyable action scemes that really embrace comic book fun.\n",
    "The villain loki is perfect as although he may have a generic motavation, the way clashes with the heroes ideologies and has huge amounts of personality made him unforgettable. \n",
    "\n",
    "One of my favourite things about this movie is the sense of urgency and dread, it really does feel like anything could happen and some thing could go wrong and this is due to some great writing and acting. another thing i love is that the characters work so well with each oher and are handled so well, the best avengers (not in power but in character complexity) are all focused on more than the more boring characters which is awesome as it feels no potential was lost in the characters and it shows they actually put thought into there characters.\n",
    "\n",
    "This movie isnt perfect though as the cinematography which was dissapointing and felt really bland and ugly to look at as it felt like they dont use there unique set design.\n",
    "\n",
    "Overall this movie still holds up as one of the best and most unforgettable mcu experiences but was still dissapionting in some aspects.'''\n",
    "test_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given review shows positive sentiment\n"
     ]
    }
   ],
   "source": [
    "review_classifier(test_review, tf_vector, nb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh god! I\\'ve never seen an overrated film like Tees maar Khan. Even imbd had given it something like 2.5 out of 10 overacting, flop comedic scenes and an useless script just ruined the film completely. And J can\\'t understand, what has happened to our Indian audience?Some of the people are even saying that \"if you don\\'t like this movie you have a terrible taste!\" I mean are you being serious? What can I say these types of movies perfectly suits Indian audience. Anyone who thinks this movie is great first watch movies like wolf of Wall Street, django unchained, inception and then watch these films, you\\'ll understand the difference between a good film and a n average film'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_test_review = '''Oh god! I've never seen an overrated film like Tees maar Khan. Even imbd had given it something like 2.5 out of 10 overacting, flop comedic scenes and an useless script just ruined the film completely. And J can't understand, what has happened to our Indian audience?Some of the people are even saying that \"if you don't like this movie you have a terrible taste!\" I mean are you being serious? What can I say these types of movies perfectly suits Indian audience. Anyone who thinks this movie is great first watch movies like wolf of Wall Street, django unchained, inception and then watch these films, you'll understand the difference between a good film and a n average film'''\n",
    "bad_test_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given review shows negative sentiment\n"
     ]
    }
   ],
   "source": [
    "review_classifier(bad_test_review, tf_vector, nb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
